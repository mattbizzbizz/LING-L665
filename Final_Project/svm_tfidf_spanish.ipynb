{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from TweetTokenizer_modified import TweetTokenizer\n",
    "import unicodedata\n",
    "import emoji\n",
    "import html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "training_data= json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\training\\\\EXIST2023_training.json\", encoding='utf-8'))\n",
    "test_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\test\\\\EXIST2023_test_clean.json\", encoding='utf-8'))\n",
    "dev_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\dev\\\\EXIST2023_dev.json\", encoding='utf-8'))\n",
    "\n",
    "train_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_training_task1_gold_hard.json\", encoding='utf-8'))\n",
    "dev_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_dev_task1_gold_hard.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matts cleanTweet Function\n",
    "# Remove duplicated word\n",
    "#     Return cleaned tweet and count of word\n",
    "def remove_duplicate(tweet, target):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    target_bool = False # Bool for whether the target exists in the tweet\n",
    "    count = 0 # Count of targets\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == target:\n",
    "\n",
    "            # Increment count if 2+ target words were present\n",
    "            #     otherwise, add target to clean_tweet and set target_bool to True\n",
    "            if target_bool:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                clean_tweet += ' ' + word\n",
    "                target_bool = True\n",
    "\n",
    "        else:\n",
    "            clean_tweet += ' ' + word\n",
    "\n",
    "    return clean_tweet.strip(), count\n",
    "\n",
    "# Replace upside-down punctation marks\n",
    "#     Return cleaned tweet\n",
    "def replace_punct(tweet, upside_down_punct, punct):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    stack = [] # Stack of punctuation marks\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == upside_down_punct: stack.append(upside_down_punct)\n",
    "\n",
    "        elif word == punct:\n",
    "\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                clean_tweet += ' ' + word\n",
    "            else: clean_tweet += ' ' + word\n",
    "\n",
    "        else: clean_tweet += ' ' + word\n",
    "\n",
    "    for elem in stack:\n",
    "        clean_tweet += ' ' + punct\n",
    "\n",
    "    return clean_tweet.strip()\n",
    "\n",
    "def normalize(tweet):\n",
    "    clean_tweet = ''\n",
    "    for char in tweet:\n",
    "        val = ord(char)\n",
    "        if val >= 119938 and val <= 120067:\n",
    "            val -= 119841\n",
    "        clean_tweet += chr(val)\n",
    "    return clean_tweet\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "\n",
    "    #print(f'Original Tweet: {tweet}')\n",
    "\n",
    "    clean_tweet = html.unescape(tweet) # Convert html characters to unicode\n",
    "    clean_tweet = unicodedata.normalize('NFKC', clean_tweet) # Normalize font\n",
    "    clean_tweet = normalize(clean_tweet) # Fix weird fonts\n",
    "\n",
    "    clean_tweet = re.sub(r'•͈ᴗ•͈', emoji.emojize(':smiling_face_with_tear:'), clean_tweet) # Convert •͈ᴗ•͈ into an emoji\n",
    "\n",
    "    clean_tweet = re.sub(r'https://[a-zA-Z0-9/.:]+', '', clean_tweet) # Remove links\n",
    "\n",
    "    clean_tweet = re.sub(r'(@[a-zA-Z]+)@([a-zA-Z])', '\\g<1> @\\g<2>', clean_tweet) # Add space between usernames\n",
    "\n",
    "    clean_tweet = re.sub(r'([^ @][a-zA-z])@([a-zA-Z])', '\\g<1>ATIDENTIFICATIONTAG\\g<2>', clean_tweet) # Turn @ into an identification tag if it is not a username\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-zA-Z ])[´`‘’]([a-zA-Z])\", \"\\g<1>'\\g<2>\", clean_tweet) # Convert ´ and ` when surrounded by letters\n",
    "    clean_tweet = re.sub(r'([0-9])°', '\\g<1> degrees', clean_tweet) # Convert ° into the word 'degrees' when directly after a number\n",
    "    clean_tweet = re.sub(r'([0-9])%', '\\g<1> percent', clean_tweet) # Convert % into the word 'percent' when directly after a number\n",
    "    clean_tweet = re.sub(r'([a-zA-Z])[*]+([a-z])', '\\g<1>astrickidentificationtag\\g<2>', clean_tweet) # Convert censoring astricks into identification tags\n",
    "\n",
    "    clean_tweet = re.sub(r'[.´`^¨~°|─­,;‘’\"“”«»()\\[\\]{}®\\$£€*%↓ِ\\u0301\\u200D]', ' ', clean_tweet) # Replace special characters with a space\n",
    "\n",
    "    clean_tweet = re.sub(r'\\u00A9\\uFE0F', 'c', clean_tweet) # DOUBLE-CHECK THIS Replacing copyrite symbol with a 'c'\n",
    "\n",
    "    clean_tweet = ' '.join(TweetTokenizer(strip_handles = True, reduce_len = True, preserve_case = False).tokenize(clean_tweet)) # Tokenise tweet\n",
    "\n",
    "    clean_tweet = re.sub(r' :($|\\s)', '\\g<1> ', clean_tweet) # Replace colons with a space when they aren't part of a time\n",
    "\n",
    "    clean_tweet = re.sub(r' / ', ' ', clean_tweet) # Remove backslashes\n",
    "    clean_tweet = re.sub(r'\\w*\\d\\w*', ' ', clean_tweet) # Remove words with numbers\n",
    "\n",
    "    clean_tweet = re.sub(r'<3', emoji.emojize(':red_heart:'), clean_tweet) # Convert <3 into an emoji\n",
    "    clean_tweet = re.sub(r'\\+', ' plus ', clean_tweet) # Convert + into the word plus\n",
    "    clean_tweet = re.sub(r'\\-', ' minus ', clean_tweet) # Convert - into the word minus\n",
    "\n",
    "    clean_tweet = re.sub(r'atidentificationtag', '@', clean_tweet) # Convert @ symbols back\n",
    "    clean_tweet = re.sub(r'astrickidentificationtag', '*', clean_tweet) # Convert * symbols back\n",
    "\n",
    "    clean_tweet = re.sub(r'[<>]', '', clean_tweet) # Remove < and >\n",
    "    clean_tweet = re.sub(r' -($|\\s)', ' ', clean_tweet) # Remove hyphens when not connecting words or numbers\n",
    "\n",
    "    hashtag_regex = re.compile('#[\\w]+')\n",
    "    hashtag_lst = hashtag_regex.findall(clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(r'#[\\w]+', '<HASHTAG>', clean_tweet) # Convert hashtags to <HASHTAG>\n",
    "    clean_tweet = re.sub(r'usernameidentificationtag', '<USERNAME>', clean_tweet) # Convert usernames to <USERNAME>\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-z>]) '[\\s]*s \", \"\\g<1>'s \", clean_tweet) # Reattach possesives\n",
    "\n",
    "    clean_tweet, username_count = remove_duplicate(clean_tweet, '<USERNAME>') # Remove duplicate <USERNAME>\n",
    "    clean_tweet, possesive_username_count = remove_duplicate(clean_tweet, \"<USERNAME>'s\") # Remove duplicate <USERNAME>'s\n",
    "    clean_tweet, hashtag_count = remove_duplicate(clean_tweet, '<HASHTAG>') # Remove duplicate <HASHTAG>\n",
    "\n",
    "    clean_tweet = replace_punct(clean_tweet, '¡', '!') # Convert upside-down exclamation points to exclamation points\n",
    "    clean_tweet = replace_punct(clean_tweet, '¿', '?') # Convert upside-down question marks to question marks\n",
    "    clean_tweet = re.sub(r'&', 'and', clean_tweet) # Convert ampersand to the word 'and'\n",
    "    clean_tweet = re.sub(r'à', 'á', clean_tweet) # Convert à to á\n",
    "    clean_tweet = re.sub(r'ª', 'a', clean_tweet) # Convert ª to a\n",
    "    clean_tweet = re.sub(r'[êė]', 'e', clean_tweet) # Convert ê to e\n",
    "    clean_tweet = re.sub(r'ò', 'ó', clean_tweet) # Convert ò to ó\n",
    "    clean_tweet = re.sub(r'ô', 'o', clean_tweet) # Convert ô to o\n",
    "\n",
    "    clean_tweet, exclamation_count = remove_duplicate(clean_tweet, '!') # Remove duplicate exclamation points\n",
    "    clean_tweet, question_count = remove_duplicate(clean_tweet, '?') # Remove duplicate exclamation points\n",
    "    clean_tweet = re.sub(r'[\\u0600-\\u06FF]', '', clean_tweet) # Remove Arabic characters\n",
    "    clean_tweet = re.sub(r'[\\u10A0-\\u10FF]+', '', clean_tweet) # Remove Gregorian characters\n",
    "    clean_tweet = re.sub(r'[\\u4E00-\\u9FFF]+', '', clean_tweet) # Remove CJK characters\n",
    "    clean_tweet = re.sub(r'[\\uAC00-\\uD7AF]+', '', clean_tweet) # Remove Hangul characters\n",
    "    clean_tweet = re.sub(r'[\\u3040-\\u309F]+', '', clean_tweet) # Remove hiragana\n",
    "    clean_tweet = re.sub(r\" '()\", \" \", clean_tweet) # Remove separated apostrophes\n",
    "    clean_tweet = re.sub(r' +', ' ', clean_tweet) # Remove double-spaces\n",
    "\n",
    "    return clean_tweet, username_count, exclamation_count, question_count, possesive_username_count, hashtag_count, hashtag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get the DIRTY training data\n",
    "span_X_train = [value['tweet'] for key, value in training_data.items() if value['lang'] == 'es']\n",
    "span_Y_train = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in training_data.items() if value['lang'] == 'es']\n",
    "\n",
    "# Get the DIRTY test data\n",
    "span_X_test = [value['tweet'] for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "span_Y_test = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train) == len(span_Y_train))\n",
    "print(len(span_X_test) == len(span_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Clean the Data\n",
    "span_X_train_clean = [cleanTweet(tweet)[0] for tweet in span_X_train]\n",
    "span_X_test_clean = [cleanTweet(tweet)[0] for tweet in span_X_test]\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train_clean) == len(span_Y_train))\n",
    "print(len(span_X_test_clean) == len(span_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty: @TheChiflis Ignora al otro, es un capullo.El problema con este youtuber denuncia el acoso... cuando no afecta a la gente de izquierdas. Por ejemplo, en su video sobre el gamergate presenta como \"normal\" el acoso que reciben Fisher, Anita o Zöey cuando hubo hasta amenazas de bomba.\n",
      "Clean: <USERNAME> ignora al otro es un capullo el problema con este youtuber denuncia el acoso cuando no afecta a la gente de izquierdas por ejemplo en su video sobre el gamergate presenta como normal el acoso que reciben fisher anita o zöey cuando hubo hasta amenazas de bomba\n",
      "-----------------------\n",
      "Dirty: @ultimonomada_ Si comicsgate se parece en algo a gamergate pues muy bien por el acoso. Y si se está haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la razón. Sois unos lloricas ofendidos.\n",
      "Clean: <USERNAME> si comicsgate se parece en algo a gamergate pues muy bien por el acoso y si se está haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la razón sois unos lloricas ofendidos\n",
      "-----------------------\n",
      "Dirty: @Steven2897 Lee sobre Gamergate, y como eso ha cambiado la manera en la cual nos comunicamos en el internet. Los fanboys de Halo están tóxicos pero los fanboys de otras comunidades/juegos también han querido coger pauta con eso 🤷🏾‍♂️\n",
      "Clean: <USERNAME> lee sobre gamergate y como eso ha cambiado la manera en la cual nos comunicamos en el internet los fanboys de halo están tóxicos pero los fanboys de otras comunidades juegos también han querido coger pauta con eso 🤷🏾 ♂ ️\n",
      "-----------------------\n",
      "Dirty: @Lunariita7 Un retraso social bastante lamentable, gamergate, trump, y aquí en españa pues lo que tenemos...\n",
      "Clean: <USERNAME> un retraso social bastante lamentable gamergate trump y aquí en españa pues lo que tenemos\n",
      "-----------------------\n",
      "Dirty: @novadragon21 @icep4ck @TvDannyZ Entonces como así es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las víctimas del gamergate.Acerca de lo otro, el \"tenían\" implica un imperativo entonces no entiendo lo del buscaban.\n",
      "Clean: <USERNAME> entonces como así es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las víctimas del gamergate acerca de lo otro el tenían implica un imperativo entonces no entiendo lo del buscaban\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Check out the data\n",
    "for dirty, clean in zip(span_X_train[:5], span_X_train_clean[:5]):\n",
    "    print('Dirty:', dirty)\n",
    "    print('Clean:', clean)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the Spanish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty training data shape (3660, 34060)\n",
      "Dirty testing data shape (549, 34060)\n"
     ]
    }
   ],
   "source": [
    "# DIRTY\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the dirty training data\n",
    "tfidf.fit(span_X_train)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_tfidf = tfidf.transform(span_X_train)\n",
    "X_test_tfidf = tfidf.transform(span_X_test)\n",
    "\n",
    "\n",
    "print('Dirty training data shape', X_train_tfidf.shape)\n",
    "print('Dirty testing data shape', X_test_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training data shape: (3660, 34060)\n",
      "Clean testing data shape: (549, 34060)\n"
     ]
    }
   ],
   "source": [
    "# CLEAN\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "clean_Tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the clean training data\n",
    "clean_Tfidf.fit(span_X_train_clean)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_clean_tfidf = tfidf.transform(span_X_train_clean)\n",
    "X_test_clean_tfidf = tfidf.transform(span_X_test_clean)\n",
    "\n",
    "\n",
    "print('Clean training data shape:', X_train_clean_tfidf.shape)\n",
    "print('Clean testing data shape:', X_test_clean_tfidf.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Spanish Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# DIRTY MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "svm_gs = GridSearchCV(svm, params)\n",
    "svm_gs.fit(X_train_tfidf, span_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "y_pred = svm_gs.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6961    0.5502    0.6146       229\n",
      "           1     0.7201    0.8281    0.7703       320\n",
      "\n",
      "    accuracy                         0.7122       549\n",
      "   macro avg     0.7081    0.6892    0.6925       549\n",
      "weighted avg     0.7101    0.7122    0.7054       549\n",
      "\n",
      "Accuracy: 0.7122040072859745\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(span_Y_test, y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(span_Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 1, 'kernel': 'poly'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.6638    0.6831       229\n",
      "           1     0.7688    0.8000    0.7841       320\n",
      "\n",
      "    accuracy                         0.7432       549\n",
      "   macro avg     0.7362    0.7319    0.7336       549\n",
      "weighted avg     0.7416    0.7432    0.7420       549\n",
      "\n",
      "Accuracy: 0.7431693989071039\n"
     ]
    }
   ],
   "source": [
    "# CLEAN MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "clean_svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "clean_svm_gs = GridSearchCV(clean_svm, params)\n",
    "clean_svm_gs.fit(X_train_clean_tfidf, span_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", clean_svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "clean_y_pred = clean_svm_gs.predict(X_test_clean_tfidf)\n",
    "\n",
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(span_Y_test, clean_y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(span_Y_test, clean_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

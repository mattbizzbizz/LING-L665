{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from TweetTokenizer_modified import TweetTokenizer\n",
    "import unicodedata\n",
    "import emoji\n",
    "import html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "training_data= json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\training\\\\EXIST2023_training.json\", encoding='utf-8'))\n",
    "test_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\test\\\\EXIST2023_test_clean.json\", encoding='utf-8'))\n",
    "dev_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\dev\\\\EXIST2023_dev.json\", encoding='utf-8'))\n",
    "\n",
    "train_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_training_task1_gold_hard.json\", encoding='utf-8'))\n",
    "dev_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_dev_task1_gold_hard.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matts cleanTweet Function\n",
    "# Remove duplicated word\n",
    "#     Return cleaned tweet and count of word\n",
    "def remove_duplicate(tweet, target):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    target_bool = False # Bool for whether the target exists in the tweet\n",
    "    count = 0 # Count of targets\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == target:\n",
    "\n",
    "            # Increment count if 2+ target words were present\n",
    "            #     otherwise, add target to clean_tweet and set target_bool to True\n",
    "            if target_bool:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                clean_tweet += ' ' + word\n",
    "                target_bool = True\n",
    "\n",
    "        else:\n",
    "            clean_tweet += ' ' + word\n",
    "\n",
    "    return clean_tweet.strip(), count\n",
    "\n",
    "# Replace upside-down punctation marks\n",
    "#     Return cleaned tweet\n",
    "def replace_punct(tweet, upside_down_punct, punct):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    stack = [] # Stack of punctuation marks\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == upside_down_punct: stack.append(upside_down_punct)\n",
    "\n",
    "        elif word == punct:\n",
    "\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                clean_tweet += ' ' + word\n",
    "            else: clean_tweet += ' ' + word\n",
    "\n",
    "        else: clean_tweet += ' ' + word\n",
    "\n",
    "    for elem in stack:\n",
    "        clean_tweet += ' ' + punct\n",
    "\n",
    "    return clean_tweet.strip()\n",
    "\n",
    "def normalize(tweet):\n",
    "    clean_tweet = ''\n",
    "    for char in tweet:\n",
    "        val = ord(char)\n",
    "        if val >= 119938 and val <= 120067:\n",
    "            val -= 119841\n",
    "        clean_tweet += chr(val)\n",
    "    return clean_tweet\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "\n",
    "    #print(f'Original Tweet: {tweet}')\n",
    "\n",
    "    clean_tweet = html.unescape(tweet) # Convert html characters to unicode\n",
    "    clean_tweet = unicodedata.normalize('NFKC', clean_tweet) # Normalize font\n",
    "    clean_tweet = normalize(clean_tweet) # Fix weird fonts\n",
    "\n",
    "    clean_tweet = re.sub(r'‚Ä¢Õà·¥ó‚Ä¢Õà', emoji.emojize(':smiling_face_with_tear:'), clean_tweet) # Convert ‚Ä¢Õà·¥ó‚Ä¢Õà into an emoji\n",
    "\n",
    "    clean_tweet = re.sub(r'https://[a-zA-Z0-9/.:]+', '', clean_tweet) # Remove links\n",
    "\n",
    "    clean_tweet = re.sub(r'(@[a-zA-Z]+)@([a-zA-Z])', '\\g<1> @\\g<2>', clean_tweet) # Add space between usernames\n",
    "\n",
    "    clean_tweet = re.sub(r'([^ @][a-zA-z])@([a-zA-Z])', '\\g<1>ATIDENTIFICATIONTAG\\g<2>', clean_tweet) # Turn @ into an identification tag if it is not a username\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-zA-Z ])[¬¥`‚Äò‚Äô]([a-zA-Z])\", \"\\g<1>'\\g<2>\", clean_tweet) # Convert ¬¥ and ` when surrounded by letters\n",
    "    clean_tweet = re.sub(r'([0-9])¬∞', '\\g<1> degrees', clean_tweet) # Convert ¬∞ into the word 'degrees' when directly after a number\n",
    "    clean_tweet = re.sub(r'([0-9])%', '\\g<1> percent', clean_tweet) # Convert % into the word 'percent' when directly after a number\n",
    "    clean_tweet = re.sub(r'([a-zA-Z])[*]+([a-z])', '\\g<1>astrickidentificationtag\\g<2>', clean_tweet) # Convert censoring astricks into identification tags\n",
    "\n",
    "    clean_tweet = re.sub(r'[.¬¥`^¬®~¬∞|‚îÄ¬≠,;‚Äò‚Äô\"‚Äú‚Äù¬´¬ª()\\[\\]{}¬Æ\\$¬£‚Ç¨*%‚ÜìŸê\\u0301\\u200D]', ' ', clean_tweet) # Replace special characters with a space\n",
    "\n",
    "    clean_tweet = re.sub(r'\\u00A9\\uFE0F', 'c', clean_tweet) # DOUBLE-CHECK THIS Replacing copyrite symbol with a 'c'\n",
    "\n",
    "    clean_tweet = ' '.join(TweetTokenizer(strip_handles = True, reduce_len = True, preserve_case = False).tokenize(clean_tweet)) # Tokenise tweet\n",
    "\n",
    "    clean_tweet = re.sub(r' :($|\\s)', '\\g<1> ', clean_tweet) # Replace colons with a space when they aren't part of a time\n",
    "\n",
    "    clean_tweet = re.sub(r' / ', ' ', clean_tweet) # Remove backslashes\n",
    "    clean_tweet = re.sub(r'\\w*\\d\\w*', ' ', clean_tweet) # Remove words with numbers\n",
    "\n",
    "    clean_tweet = re.sub(r'<3', emoji.emojize(':red_heart:'), clean_tweet) # Convert <3 into an emoji\n",
    "    clean_tweet = re.sub(r'\\+', ' plus ', clean_tweet) # Convert + into the word plus\n",
    "    clean_tweet = re.sub(r'\\-', ' minus ', clean_tweet) # Convert - into the word minus\n",
    "\n",
    "    clean_tweet = re.sub(r'atidentificationtag', '@', clean_tweet) # Convert @ symbols back\n",
    "    clean_tweet = re.sub(r'astrickidentificationtag', '*', clean_tweet) # Convert * symbols back\n",
    "\n",
    "    clean_tweet = re.sub(r'[<>]', '', clean_tweet) # Remove < and >\n",
    "    clean_tweet = re.sub(r' -($|\\s)', ' ', clean_tweet) # Remove hyphens when not connecting words or numbers\n",
    "\n",
    "    hashtag_regex = re.compile('#[\\w]+')\n",
    "    hashtag_lst = hashtag_regex.findall(clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(r'#[\\w]+', '<HASHTAG>', clean_tweet) # Convert hashtags to <HASHTAG>\n",
    "    clean_tweet = re.sub(r'usernameidentificationtag', '<USERNAME>', clean_tweet) # Convert usernames to <USERNAME>\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-z>]) '[\\s]*s \", \"\\g<1>'s \", clean_tweet) # Reattach possesives\n",
    "\n",
    "    clean_tweet, username_count = remove_duplicate(clean_tweet, '<USERNAME>') # Remove duplicate <USERNAME>\n",
    "    clean_tweet, possesive_username_count = remove_duplicate(clean_tweet, \"<USERNAME>'s\") # Remove duplicate <USERNAME>'s\n",
    "    clean_tweet, hashtag_count = remove_duplicate(clean_tweet, '<HASHTAG>') # Remove duplicate <HASHTAG>\n",
    "\n",
    "    clean_tweet = replace_punct(clean_tweet, '¬°', '!') # Convert upside-down exclamation points to exclamation points\n",
    "    clean_tweet = replace_punct(clean_tweet, '¬ø', '?') # Convert upside-down question marks to question marks\n",
    "    clean_tweet = re.sub(r'&', 'and', clean_tweet) # Convert ampersand to the word 'and'\n",
    "    clean_tweet = re.sub(r'√†', '√°', clean_tweet) # Convert √† to √°\n",
    "    clean_tweet = re.sub(r'¬™', 'a', clean_tweet) # Convert ¬™ to a\n",
    "    clean_tweet = re.sub(r'[√™ƒó]', 'e', clean_tweet) # Convert √™ to e\n",
    "    clean_tweet = re.sub(r'√≤', '√≥', clean_tweet) # Convert √≤ to √≥\n",
    "    clean_tweet = re.sub(r'√¥', 'o', clean_tweet) # Convert √¥ to o\n",
    "\n",
    "    clean_tweet, exclamation_count = remove_duplicate(clean_tweet, '!') # Remove duplicate exclamation points\n",
    "    clean_tweet, question_count = remove_duplicate(clean_tweet, '?') # Remove duplicate exclamation points\n",
    "    clean_tweet = re.sub(r'[\\u0600-\\u06FF]', '', clean_tweet) # Remove Arabic characters\n",
    "    clean_tweet = re.sub(r'[\\u10A0-\\u10FF]+', '', clean_tweet) # Remove Gregorian characters\n",
    "    clean_tweet = re.sub(r'[\\u4E00-\\u9FFF]+', '', clean_tweet) # Remove CJK characters\n",
    "    clean_tweet = re.sub(r'[\\uAC00-\\uD7AF]+', '', clean_tweet) # Remove Hangul characters\n",
    "    clean_tweet = re.sub(r'[\\u3040-\\u309F]+', '', clean_tweet) # Remove hiragana\n",
    "    clean_tweet = re.sub(r\" '()\", \" \", clean_tweet) # Remove separated apostrophes\n",
    "    clean_tweet = re.sub(r' +', ' ', clean_tweet) # Remove double-spaces\n",
    "\n",
    "    return clean_tweet, username_count, exclamation_count, question_count, possesive_username_count, hashtag_count, hashtag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get the DIRTY training data\n",
    "span_X_train = [value['tweet'] for key, value in training_data.items() if value['lang'] == 'es']\n",
    "span_Y_train = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in training_data.items() if value['lang'] == 'es']\n",
    "\n",
    "# Get the DIRTY test data\n",
    "span_X_test = [value['tweet'] for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "span_Y_test = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train) == len(span_Y_train))\n",
    "print(len(span_X_test) == len(span_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Clean the Data\n",
    "span_X_train_clean = [cleanTweet(tweet)[0] for tweet in span_X_train]\n",
    "span_X_test_clean = [cleanTweet(tweet)[0] for tweet in span_X_test]\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train_clean) == len(span_Y_train))\n",
    "print(len(span_X_test_clean) == len(span_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty: @TheChiflis Ignora al otro, es un capullo.El problema con este youtuber denuncia el acoso... cuando no afecta a la gente de izquierdas. Por ejemplo, en su video sobre el gamergate presenta como \"normal\" el acoso que reciben Fisher, Anita o Z√∂ey cuando hubo hasta amenazas de bomba.\n",
      "Clean: <USERNAME> ignora al otro es un capullo el problema con este youtuber denuncia el acoso cuando no afecta a la gente de izquierdas por ejemplo en su video sobre el gamergate presenta como normal el acoso que reciben fisher anita o z√∂ey cuando hubo hasta amenazas de bomba\n",
      "-----------------------\n",
      "Dirty: @ultimonomada_ Si comicsgate se parece en algo a gamergate pues muy bien por el acoso. Y si se est√° haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la raz√≥n. Sois unos lloricas ofendidos.\n",
      "Clean: <USERNAME> si comicsgate se parece en algo a gamergate pues muy bien por el acoso y si se est√° haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la raz√≥n sois unos lloricas ofendidos\n",
      "-----------------------\n",
      "Dirty: @Steven2897 Lee sobre Gamergate, y como eso ha cambiado la manera en la cual nos comunicamos en el internet. Los fanboys de Halo est√°n t√≥xicos pero los fanboys de otras comunidades/juegos tambi√©n han querido coger pauta con eso ü§∑üèæ‚Äç‚ôÇÔ∏è\n",
      "Clean: <USERNAME> lee sobre gamergate y como eso ha cambiado la manera en la cual nos comunicamos en el internet los fanboys de halo est√°n t√≥xicos pero los fanboys de otras comunidades juegos tambi√©n han querido coger pauta con eso ü§∑üèæ ‚ôÇ Ô∏è\n",
      "-----------------------\n",
      "Dirty: @Lunariita7 Un retraso social bastante lamentable, gamergate, trump, y aqu√≠ en espa√±a pues lo que tenemos...\n",
      "Clean: <USERNAME> un retraso social bastante lamentable gamergate trump y aqu√≠ en espa√±a pues lo que tenemos\n",
      "-----------------------\n",
      "Dirty: @novadragon21 @icep4ck @TvDannyZ Entonces como as√≠ es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las v√≠ctimas del gamergate.Acerca de lo otro, el \"ten√≠an\" implica un imperativo entonces no entiendo lo del buscaban.\n",
      "Clean: <USERNAME> entonces como as√≠ es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las v√≠ctimas del gamergate acerca de lo otro el ten√≠an implica un imperativo entonces no entiendo lo del buscaban\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Check out the data\n",
    "for dirty, clean in zip(span_X_train[:5], span_X_train_clean[:5]):\n",
    "    print('Dirty:', dirty)\n",
    "    print('Clean:', clean)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the Spanish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty training data shape (3660, 34060)\n",
      "Dirty testing data shape (549, 34060)\n"
     ]
    }
   ],
   "source": [
    "# DIRTY\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the dirty training data\n",
    "tfidf.fit(span_X_train)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_tfidf = tfidf.transform(span_X_train)\n",
    "X_test_tfidf = tfidf.transform(span_X_test)\n",
    "\n",
    "\n",
    "print('Dirty training data shape', X_train_tfidf.shape)\n",
    "print('Dirty testing data shape', X_test_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training data shape: (3660, 34060)\n",
      "Clean testing data shape: (549, 34060)\n"
     ]
    }
   ],
   "source": [
    "# CLEAN\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "clean_Tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the clean training data\n",
    "clean_Tfidf.fit(span_X_train_clean)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_clean_tfidf = tfidf.transform(span_X_train_clean)\n",
    "X_test_clean_tfidf = tfidf.transform(span_X_test_clean)\n",
    "\n",
    "\n",
    "print('Clean training data shape:', X_train_clean_tfidf.shape)\n",
    "print('Clean testing data shape:', X_test_clean_tfidf.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Spanish Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# DIRTY MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "svm_gs = GridSearchCV(svm, params)\n",
    "svm_gs.fit(X_train_tfidf, span_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "y_pred = svm_gs.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6961    0.5502    0.6146       229\n",
      "           1     0.7201    0.8281    0.7703       320\n",
      "\n",
      "    accuracy                         0.7122       549\n",
      "   macro avg     0.7081    0.6892    0.6925       549\n",
      "weighted avg     0.7101    0.7122    0.7054       549\n",
      "\n",
      "Accuracy: 0.7122040072859745\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(span_Y_test, y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(span_Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 1, 'kernel': 'poly'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.6638    0.6831       229\n",
      "           1     0.7688    0.8000    0.7841       320\n",
      "\n",
      "    accuracy                         0.7432       549\n",
      "   macro avg     0.7362    0.7319    0.7336       549\n",
      "weighted avg     0.7416    0.7432    0.7420       549\n",
      "\n",
      "Accuracy: 0.7431693989071039\n"
     ]
    }
   ],
   "source": [
    "# CLEAN MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "clean_svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "clean_svm_gs = GridSearchCV(clean_svm, params)\n",
    "clean_svm_gs.fit(X_train_clean_tfidf, span_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", clean_svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "clean_y_pred = clean_svm_gs.predict(X_test_clean_tfidf)\n",
    "\n",
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(span_Y_test, clean_y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(span_Y_test, clean_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

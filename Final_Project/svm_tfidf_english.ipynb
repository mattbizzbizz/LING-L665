{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from TweetTokenizer_modified import TweetTokenizer\n",
    "import unicodedata\n",
    "import emoji\n",
    "import html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "training_data= json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\training\\\\EXIST2023_training.json\", encoding='utf-8'))\n",
    "test_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\test\\\\EXIST2023_test_clean.json\", encoding='utf-8'))\n",
    "dev_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\dev\\\\EXIST2023_dev.json\", encoding='utf-8'))\n",
    "\n",
    "train_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_training_task1_gold_hard.json\", encoding='utf-8'))\n",
    "dev_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_dev_task1_gold_hard.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matts cleanTweet Function\n",
    "# Remove duplicated word\n",
    "#     Return cleaned tweet and count of word\n",
    "def remove_duplicate(tweet, target):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    target_bool = False # Bool for whether the target exists in the tweet\n",
    "    count = 0 # Count of targets\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == target:\n",
    "\n",
    "            # Increment count if 2+ target words were present\n",
    "            #     otherwise, add target to clean_tweet and set target_bool to True\n",
    "            if target_bool:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                clean_tweet += ' ' + word\n",
    "                target_bool = True\n",
    "\n",
    "        else:\n",
    "            clean_tweet += ' ' + word\n",
    "\n",
    "    return clean_tweet.strip(), count\n",
    "\n",
    "# Replace upside-down punctation marks\n",
    "#     Return cleaned tweet\n",
    "def replace_punct(tweet, upside_down_punct, punct):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    stack = [] # Stack of punctuation marks\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == upside_down_punct: stack.append(upside_down_punct)\n",
    "\n",
    "        elif word == punct:\n",
    "\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                clean_tweet += ' ' + word\n",
    "            else: clean_tweet += ' ' + word\n",
    "\n",
    "        else: clean_tweet += ' ' + word\n",
    "\n",
    "    for elem in stack:\n",
    "        clean_tweet += ' ' + punct\n",
    "\n",
    "    return clean_tweet.strip()\n",
    "\n",
    "def normalize(tweet):\n",
    "    clean_tweet = ''\n",
    "    for char in tweet:\n",
    "        val = ord(char)\n",
    "        if val >= 119938 and val <= 120067:\n",
    "            val -= 119841\n",
    "        clean_tweet += chr(val)\n",
    "    return clean_tweet\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "\n",
    "    #print(f'Original Tweet: {tweet}')\n",
    "\n",
    "    clean_tweet = html.unescape(tweet) # Convert html characters to unicode\n",
    "    clean_tweet = unicodedata.normalize('NFKC', clean_tweet) # Normalize font\n",
    "    clean_tweet = normalize(clean_tweet) # Fix weird fonts\n",
    "\n",
    "    clean_tweet = re.sub(r'‚Ä¢Õà·¥ó‚Ä¢Õà', emoji.emojize(':smiling_face_with_tear:'), clean_tweet) # Convert ‚Ä¢Õà·¥ó‚Ä¢Õà into an emoji\n",
    "\n",
    "    clean_tweet = re.sub(r'https://[a-zA-Z0-9/.:]+', '', clean_tweet) # Remove links\n",
    "\n",
    "    clean_tweet = re.sub(r'(@[a-zA-Z]+)@([a-zA-Z])', '\\g<1> @\\g<2>', clean_tweet) # Add space between usernames\n",
    "\n",
    "    clean_tweet = re.sub(r'([^ @][a-zA-z])@([a-zA-Z])', '\\g<1>ATIDENTIFICATIONTAG\\g<2>', clean_tweet) # Turn @ into an identification tag if it is not a username\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-zA-Z ])[¬¥`‚Äò‚Äô]([a-zA-Z])\", \"\\g<1>'\\g<2>\", clean_tweet) # Convert ¬¥ and ` when surrounded by letters\n",
    "    clean_tweet = re.sub(r'([0-9])¬∞', '\\g<1> degrees', clean_tweet) # Convert ¬∞ into the word 'degrees' when directly after a number\n",
    "    clean_tweet = re.sub(r'([0-9])%', '\\g<1> percent', clean_tweet) # Convert % into the word 'percent' when directly after a number\n",
    "    clean_tweet = re.sub(r'([a-zA-Z])[*]+([a-z])', '\\g<1>astrickidentificationtag\\g<2>', clean_tweet) # Convert censoring astricks into identification tags\n",
    "\n",
    "    clean_tweet = re.sub(r'[.¬¥`^¬®~¬∞|‚îÄ¬≠,;‚Äò‚Äô\"‚Äú‚Äù¬´¬ª()\\[\\]{}¬Æ\\$¬£‚Ç¨*%‚ÜìŸê\\u0301\\u200D]', ' ', clean_tweet) # Replace special characters with a space\n",
    "\n",
    "    clean_tweet = re.sub(r'\\u00A9\\uFE0F', 'c', clean_tweet) # DOUBLE-CHECK THIS Replacing copyrite symbol with a 'c'\n",
    "\n",
    "    clean_tweet = ' '.join(TweetTokenizer(strip_handles = True, reduce_len = True, preserve_case = False).tokenize(clean_tweet)) # Tokenise tweet\n",
    "\n",
    "    clean_tweet = re.sub(r' :($|\\s)', '\\g<1> ', clean_tweet) # Replace colons with a space when they aren't part of a time\n",
    "\n",
    "    clean_tweet = re.sub(r' / ', ' ', clean_tweet) # Remove backslashes\n",
    "    clean_tweet = re.sub(r'\\w*\\d\\w*', ' ', clean_tweet) # Remove words with numbers\n",
    "\n",
    "    clean_tweet = re.sub(r'<3', emoji.emojize(':red_heart:'), clean_tweet) # Convert <3 into an emoji\n",
    "    clean_tweet = re.sub(r'\\+', ' plus ', clean_tweet) # Convert + into the word plus\n",
    "    clean_tweet = re.sub(r'\\-', ' minus ', clean_tweet) # Convert - into the word minus\n",
    "\n",
    "    clean_tweet = re.sub(r'atidentificationtag', '@', clean_tweet) # Convert @ symbols back\n",
    "    clean_tweet = re.sub(r'astrickidentificationtag', '*', clean_tweet) # Convert * symbols back\n",
    "\n",
    "    clean_tweet = re.sub(r'[<>]', '', clean_tweet) # Remove < and >\n",
    "    clean_tweet = re.sub(r' -($|\\s)', ' ', clean_tweet) # Remove hyphens when not connecting words or numbers\n",
    "\n",
    "    hashtag_regex = re.compile('#[\\w]+')\n",
    "    hashtag_lst = hashtag_regex.findall(clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(r'#[\\w]+', '<HASHTAG>', clean_tweet) # Convert hashtags to <HASHTAG>\n",
    "    clean_tweet = re.sub(r'usernameidentificationtag', '<USERNAME>', clean_tweet) # Convert usernames to <USERNAME>\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-z>]) '[\\s]*s \", \"\\g<1>'s \", clean_tweet) # Reattach possesives\n",
    "\n",
    "    clean_tweet, username_count = remove_duplicate(clean_tweet, '<USERNAME>') # Remove duplicate <USERNAME>\n",
    "    clean_tweet, possesive_username_count = remove_duplicate(clean_tweet, \"<USERNAME>'s\") # Remove duplicate <USERNAME>'s\n",
    "    clean_tweet, hashtag_count = remove_duplicate(clean_tweet, '<HASHTAG>') # Remove duplicate <HASHTAG>\n",
    "\n",
    "    clean_tweet = replace_punct(clean_tweet, '¬°', '!') # Convert upside-down exclamation points to exclamation points\n",
    "    clean_tweet = replace_punct(clean_tweet, '¬ø', '?') # Convert upside-down question marks to question marks\n",
    "    clean_tweet = re.sub(r'&', 'and', clean_tweet) # Convert ampersand to the word 'and'\n",
    "    clean_tweet = re.sub(r'√†', '√°', clean_tweet) # Convert √† to √°\n",
    "    clean_tweet = re.sub(r'¬™', 'a', clean_tweet) # Convert ¬™ to a\n",
    "    clean_tweet = re.sub(r'[√™ƒó]', 'e', clean_tweet) # Convert √™ to e\n",
    "    clean_tweet = re.sub(r'√≤', '√≥', clean_tweet) # Convert √≤ to √≥\n",
    "    clean_tweet = re.sub(r'√¥', 'o', clean_tweet) # Convert √¥ to o\n",
    "\n",
    "    clean_tweet, exclamation_count = remove_duplicate(clean_tweet, '!') # Remove duplicate exclamation points\n",
    "    clean_tweet, question_count = remove_duplicate(clean_tweet, '?') # Remove duplicate exclamation points\n",
    "    clean_tweet = re.sub(r'[\\u0600-\\u06FF]', '', clean_tweet) # Remove Arabic characters\n",
    "    clean_tweet = re.sub(r'[\\u10A0-\\u10FF]+', '', clean_tweet) # Remove Gregorian characters\n",
    "    clean_tweet = re.sub(r'[\\u4E00-\\u9FFF]+', '', clean_tweet) # Remove CJK characters\n",
    "    clean_tweet = re.sub(r'[\\uAC00-\\uD7AF]+', '', clean_tweet) # Remove Hangul characters\n",
    "    clean_tweet = re.sub(r'[\\u3040-\\u309F]+', '', clean_tweet) # Remove hiragana\n",
    "    clean_tweet = re.sub(r\" '()\", \" \", clean_tweet) # Remove separated apostrophes\n",
    "    clean_tweet = re.sub(r' +', ' ', clean_tweet) # Remove double-spaces\n",
    "\n",
    "    return clean_tweet, username_count, exclamation_count, question_count, possesive_username_count, hashtag_count, hashtag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get the DIRTY training data\n",
    "eng_X_train = [value['tweet'] for key, value in training_data.items() if value['lang'] == 'en']\n",
    "eng_Y_train = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in training_data.items() if value['lang'] == 'en']\n",
    "\n",
    "# Get the DIRTY test data\n",
    "eng_X_test = [value['tweet'] for key, value in dev_data.items() if value['lang'] == 'en']\n",
    "eng_Y_test = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in dev_data.items() if value['lang'] == 'en']\n",
    "\n",
    "# Sanity Check\n",
    "print(len(eng_X_train) == len(eng_Y_train))\n",
    "print(len(eng_X_test) == len(eng_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Clean the Data\n",
    "eng_X_train_clean = [cleanTweet(tweet)[0] for tweet in eng_X_train]\n",
    "eng_X_test_clean = [cleanTweet(tweet)[0] for tweet in eng_X_test]\n",
    "\n",
    "# Sanity Check\n",
    "print(len(eng_X_train_clean) == len(eng_Y_train))\n",
    "print(len(eng_X_test_clean) == len(eng_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty: FFS! How about laying the blame on the bastard who murdered her? Novel idea, I know. https://t.co/GI5B45THvJ\n",
      "Clean: ffs ! how about laying the blame on the bastard who murdered her ? novel idea i know\n",
      "-----------------------\n",
      "Dirty: Writing a uni essay in my local pub with a coffee. Random old man keeps asking me drunk questions when I'm trying to concentrate &amp; ends with \"good luck, but you'll just end up getting married and not use it anyway\". #EverydaySexism is alive and well üôÉ\n",
      "Clean: writing a uni essay in my local pub with a coffee random old man keeps asking me drunk questions when i'm trying to concentrate and ends with good luck but you'll just end up getting married and not use it anyway <HASHTAG> is alive and well üôÉ\n",
      "-----------------------\n",
      "Dirty: @UniversalORL it is 2021 not 1921. I dont appreciate that on two rides by myself your team member looked behind me and asked the man behind how many in my party. Not impressed #everydaysexism\n",
      "Clean: <USERNAME> it is not i dont appreciate that on two rides by myself your team member looked behind me and asked the man behind how many in my party not impressed <HASHTAG>\n",
      "-----------------------\n",
      "Dirty: @GMB this is unacceptable. Use her title as you did for all the men interviewed. She is, in fact, senior to Angus #everydaysexism @TheWomensOrg https://t.co/JZF3a5E4eX\n",
      "Clean: <USERNAME> this is unacceptable use her title as you did for all the men interviewed she is in fact senior to angus <HASHTAG>\n",
      "-----------------------\n",
      "Dirty: ‚ÄòMaking yourself a harder target‚Äô basically boils down to ‚Äòmake sure they target someone else‚Äô üôÉ https://t.co/VOpu09YAj6\n",
      "Clean: making yourself a harder target basically boils down to make sure they target someone else üôÉ\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Check out the data\n",
    "for dirty, clean in zip(eng_X_train[:5], eng_X_train_clean[:5]):\n",
    "    print('Dirty:', dirty)\n",
    "    print('Clean:', clean)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the English Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty training data shape (3260, 31729)\n",
      "Dirty testing data shape (489, 31729)\n"
     ]
    }
   ],
   "source": [
    "# DIRTY\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the dirty training data\n",
    "tfidf.fit(eng_X_train)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_tfidf = tfidf.transform(eng_X_train)\n",
    "X_test_tfidf = tfidf.transform(eng_X_test)\n",
    "\n",
    "\n",
    "print('Dirty training data shape', X_train_tfidf.shape)\n",
    "print('Dirty testing data shape', X_test_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training data shape: (3260, 31729)\n",
      "Clean testing data shape: (489, 31729)\n"
     ]
    }
   ],
   "source": [
    "# CLEAN\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "clean_Tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the clean training data\n",
    "clean_Tfidf.fit(eng_X_train_clean)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_clean_tfidf = tfidf.transform(eng_X_train_clean)\n",
    "X_test_clean_tfidf = tfidf.transform(eng_X_test_clean)\n",
    "\n",
    "\n",
    "print('Clean training data shape:', X_train_clean_tfidf.shape)\n",
    "print('Clean testing data shape:', X_test_clean_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the English Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# DIRTY MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "svm_gs = GridSearchCV(svm, params)\n",
    "svm_gs.fit(X_train_tfidf, eng_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "y_pred = svm_gs.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6964    0.8440    0.7631       250\n",
      "           1     0.7903    0.6151    0.6918       239\n",
      "\n",
      "    accuracy                         0.7321       489\n",
      "   macro avg     0.7433    0.7295    0.7274       489\n",
      "weighted avg     0.7423    0.7321    0.7282       489\n",
      "\n",
      "Accuracy: 0.7321063394683026\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(eng_Y_test, y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(eng_Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6528    0.8800    0.7496       250\n",
      "           1     0.8026    0.5105    0.6240       239\n",
      "\n",
      "    accuracy                         0.6994       489\n",
      "   macro avg     0.7277    0.6952    0.6868       489\n",
      "weighted avg     0.7260    0.6994    0.6882       489\n",
      "\n",
      "Accuracy: 0.6993865030674846\n"
     ]
    }
   ],
   "source": [
    "# CLEAN MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "clean_svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "clean_svm_gs = GridSearchCV(clean_svm, params)\n",
    "clean_svm_gs.fit(X_train_clean_tfidf, eng_Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", clean_svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "clean_y_pred = clean_svm_gs.predict(X_test_clean_tfidf)\n",
    "\n",
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(eng_Y_test, clean_y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(eng_Y_test, clean_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

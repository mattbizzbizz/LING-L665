{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from TweetTokenizer_modified import TweetTokenizer\n",
    "import unicodedata\n",
    "import emoji\n",
    "import html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "training_data= json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\training\\\\EXIST2023_training.json\", encoding='utf-8'))\n",
    "test_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\test\\\\EXIST2023_test_clean.json\", encoding='utf-8'))\n",
    "dev_data = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\dirty_data\\\\dev\\\\EXIST2023_dev.json\", encoding='utf-8'))\n",
    "\n",
    "train_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_training_task1_gold_hard.json\", encoding='utf-8'))\n",
    "dev_gold_hard = json.load(open(\"C:\\\\Users\\\\Jerem\\\\Documents\\\\Spring 2023\\\\EXIST\\\\evaluation\\\\golds\\\\EXIST2023_dev_task1_gold_hard.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matts cleanTweet Function\n",
    "# Remove duplicated word\n",
    "#     Return cleaned tweet and count of word\n",
    "def remove_duplicate(tweet, target):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    target_bool = False # Bool for whether the target exists in the tweet\n",
    "    count = 0 # Count of targets\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == target:\n",
    "\n",
    "            # Increment count if 2+ target words were present\n",
    "            #     otherwise, add target to clean_tweet and set target_bool to True\n",
    "            if target_bool:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                clean_tweet += ' ' + word\n",
    "                target_bool = True\n",
    "\n",
    "        else:\n",
    "            clean_tweet += ' ' + word\n",
    "\n",
    "    return clean_tweet.strip(), count\n",
    "\n",
    "# Replace upside-down punctation marks\n",
    "#     Return cleaned tweet\n",
    "def replace_punct(tweet, upside_down_punct, punct):\n",
    "\n",
    "    # Split input string separated by space\n",
    "    tweet = tweet.split(\" \")\n",
    "\n",
    "    clean_tweet = '' # Tweet with additional targets removed\n",
    "\n",
    "    stack = [] # Stack of punctuation marks\n",
    "\n",
    "    # Iterate through words in tweet\n",
    "    for word in tweet:\n",
    "\n",
    "        # Check if word is target\n",
    "        #     If it is not, add word to clean_tweet\n",
    "        if word == upside_down_punct: stack.append(upside_down_punct)\n",
    "\n",
    "        elif word == punct:\n",
    "\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                clean_tweet += ' ' + word\n",
    "            else: clean_tweet += ' ' + word\n",
    "\n",
    "        else: clean_tweet += ' ' + word\n",
    "\n",
    "    for elem in stack:\n",
    "        clean_tweet += ' ' + punct\n",
    "\n",
    "    return clean_tweet.strip()\n",
    "\n",
    "def normalize(tweet):\n",
    "    clean_tweet = ''\n",
    "    for char in tweet:\n",
    "        val = ord(char)\n",
    "        if val >= 119938 and val <= 120067:\n",
    "            val -= 119841\n",
    "        clean_tweet += chr(val)\n",
    "    return clean_tweet\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "\n",
    "    #print(f'Original Tweet: {tweet}')\n",
    "\n",
    "    clean_tweet = html.unescape(tweet) # Convert html characters to unicode\n",
    "    clean_tweet = unicodedata.normalize('NFKC', clean_tweet) # Normalize font\n",
    "    clean_tweet = normalize(clean_tweet) # Fix weird fonts\n",
    "\n",
    "    clean_tweet = re.sub(r'•͈ᴗ•͈', emoji.emojize(':smiling_face_with_tear:'), clean_tweet) # Convert •͈ᴗ•͈ into an emoji\n",
    "\n",
    "    clean_tweet = re.sub(r'https://[a-zA-Z0-9/.:]+', '', clean_tweet) # Remove links\n",
    "\n",
    "    clean_tweet = re.sub(r'(@[a-zA-Z]+)@([a-zA-Z])', '\\g<1> @\\g<2>', clean_tweet) # Add space between usernames\n",
    "\n",
    "    clean_tweet = re.sub(r'([^ @][a-zA-z])@([a-zA-Z])', '\\g<1>ATIDENTIFICATIONTAG\\g<2>', clean_tweet) # Turn @ into an identification tag if it is not a username\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-zA-Z ])[´`‘’]([a-zA-Z])\", \"\\g<1>'\\g<2>\", clean_tweet) # Convert ´ and ` when surrounded by letters\n",
    "    clean_tweet = re.sub(r'([0-9])°', '\\g<1> degrees', clean_tweet) # Convert ° into the word 'degrees' when directly after a number\n",
    "    clean_tweet = re.sub(r'([0-9])%', '\\g<1> percent', clean_tweet) # Convert % into the word 'percent' when directly after a number\n",
    "    clean_tweet = re.sub(r'([a-zA-Z])[*]+([a-z])', '\\g<1>astrickidentificationtag\\g<2>', clean_tweet) # Convert censoring astricks into identification tags\n",
    "\n",
    "    clean_tweet = re.sub(r'[.´`^¨~°|─­,;‘’\"“”«»()\\[\\]{}®\\$£€*%↓ِ\\u0301\\u200D]', ' ', clean_tweet) # Replace special characters with a space\n",
    "\n",
    "    clean_tweet = re.sub(r'\\u00A9\\uFE0F', 'c', clean_tweet) # DOUBLE-CHECK THIS Replacing copyrite symbol with a 'c'\n",
    "\n",
    "    clean_tweet = ' '.join(TweetTokenizer(strip_handles = True, reduce_len = True, preserve_case = False).tokenize(clean_tweet)) # Tokenise tweet\n",
    "\n",
    "    clean_tweet = re.sub(r' :($|\\s)', '\\g<1> ', clean_tweet) # Replace colons with a space when they aren't part of a time\n",
    "\n",
    "    clean_tweet = re.sub(r' / ', ' ', clean_tweet) # Remove backslashes\n",
    "    clean_tweet = re.sub(r'\\w*\\d\\w*', ' ', clean_tweet) # Remove words with numbers\n",
    "\n",
    "    clean_tweet = re.sub(r'<3', emoji.emojize(':red_heart:'), clean_tweet) # Convert <3 into an emoji\n",
    "    clean_tweet = re.sub(r'\\+', ' plus ', clean_tweet) # Convert + into the word plus\n",
    "    clean_tweet = re.sub(r'\\-', ' minus ', clean_tweet) # Convert - into the word minus\n",
    "\n",
    "    clean_tweet = re.sub(r'atidentificationtag', '@', clean_tweet) # Convert @ symbols back\n",
    "    clean_tweet = re.sub(r'astrickidentificationtag', '*', clean_tweet) # Convert * symbols back\n",
    "\n",
    "    clean_tweet = re.sub(r'[<>]', '', clean_tweet) # Remove < and >\n",
    "    clean_tweet = re.sub(r' -($|\\s)', ' ', clean_tweet) # Remove hyphens when not connecting words or numbers\n",
    "\n",
    "    hashtag_regex = re.compile('#[\\w]+')\n",
    "    hashtag_lst = hashtag_regex.findall(clean_tweet)\n",
    "\n",
    "    clean_tweet = re.sub(r'#[\\w]+', '<HASHTAG>', clean_tweet) # Convert hashtags to <HASHTAG>\n",
    "    clean_tweet = re.sub(r'usernameidentificationtag', '<USERNAME>', clean_tweet) # Convert usernames to <USERNAME>\n",
    "\n",
    "    clean_tweet = re.sub(r\"([a-z>]) '[\\s]*s \", \"\\g<1>'s \", clean_tweet) # Reattach possesives\n",
    "\n",
    "    clean_tweet, username_count = remove_duplicate(clean_tweet, '<USERNAME>') # Remove duplicate <USERNAME>\n",
    "    clean_tweet, possesive_username_count = remove_duplicate(clean_tweet, \"<USERNAME>'s\") # Remove duplicate <USERNAME>'s\n",
    "    clean_tweet, hashtag_count = remove_duplicate(clean_tweet, '<HASHTAG>') # Remove duplicate <HASHTAG>\n",
    "\n",
    "    clean_tweet = replace_punct(clean_tweet, '¡', '!') # Convert upside-down exclamation points to exclamation points\n",
    "    clean_tweet = replace_punct(clean_tweet, '¿', '?') # Convert upside-down question marks to question marks\n",
    "    clean_tweet = re.sub(r'&', 'and', clean_tweet) # Convert ampersand to the word 'and'\n",
    "    clean_tweet = re.sub(r'à', 'á', clean_tweet) # Convert à to á\n",
    "    clean_tweet = re.sub(r'ª', 'a', clean_tweet) # Convert ª to a\n",
    "    clean_tweet = re.sub(r'[êė]', 'e', clean_tweet) # Convert ê to e\n",
    "    clean_tweet = re.sub(r'ò', 'ó', clean_tweet) # Convert ò to ó\n",
    "    clean_tweet = re.sub(r'ô', 'o', clean_tweet) # Convert ô to o\n",
    "\n",
    "    clean_tweet, exclamation_count = remove_duplicate(clean_tweet, '!') # Remove duplicate exclamation points\n",
    "    clean_tweet, question_count = remove_duplicate(clean_tweet, '?') # Remove duplicate exclamation points\n",
    "    clean_tweet = re.sub(r'[\\u0600-\\u06FF]', '', clean_tweet) # Remove Arabic characters\n",
    "    clean_tweet = re.sub(r'[\\u10A0-\\u10FF]+', '', clean_tweet) # Remove Gregorian characters\n",
    "    clean_tweet = re.sub(r'[\\u4E00-\\u9FFF]+', '', clean_tweet) # Remove CJK characters\n",
    "    clean_tweet = re.sub(r'[\\uAC00-\\uD7AF]+', '', clean_tweet) # Remove Hangul characters\n",
    "    clean_tweet = re.sub(r'[\\u3040-\\u309F]+', '', clean_tweet) # Remove hiragana\n",
    "    clean_tweet = re.sub(r\" '()\", \" \", clean_tweet) # Remove separated apostrophes\n",
    "    clean_tweet = re.sub(r' +', ' ', clean_tweet) # Remove double-spaces\n",
    "\n",
    "    return clean_tweet, username_count, exclamation_count, question_count, possesive_username_count, hashtag_count, hashtag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get the DIRTY training data\n",
    "span_X_train = [value['tweet'] for key, value in training_data.items() if value['lang'] == 'es']\n",
    "span_train_IDs = [value['id_EXIST'] for key, value in training_data.items() if value['lang'] == 'es']\n",
    "span_Y_train = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in training_data.items() if value['lang'] == 'es']\n",
    "\n",
    "\n",
    "span_train_sexist = [span_X_train[i] for i in range(len(span_Y_train)) if span_Y_train[i] == 1]\n",
    "span_train_sexist_IDs = [span_train_IDs[i] for i in range(len(span_Y_train)) if span_Y_train[i] == 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the DIRTY test data\n",
    "span_X_test = [value['tweet'] for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "span_test_IDs = [value['id_EXIST'] for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "span_Y_test = [1 if value['labels_task1'].count('YES') >= 3 else 0 \n",
    "               for key, value in dev_data.items() if value['lang'] == 'es']\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train) == len(span_Y_train))\n",
    "print(len(span_X_test) == len(span_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Clean the Data\n",
    "span_X_train_clean = [cleanTweet(tweet)[:5] for tweet in span_X_train]\n",
    "span_X_test_clean = [cleanTweet(tweet)[:5] for tweet in span_X_test]\n",
    "\n",
    "# Sanity Check\n",
    "print(len(span_X_train_clean) == len(span_Y_train))\n",
    "print(len(span_X_test_clean) == len(span_Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sexist before upsampling: 2026\n",
      "Not sexist before upsampling: 1634\n",
      "Sexist after upsampling: 4052\n",
      "Not sexist after upsampling: 1634\n"
     ]
    }
   ],
   "source": [
    "# Upsample the clean sexist tweets by adding them back into X_train, do the same for the labels, and then do the same for the IDs\n",
    "\n",
    "# Get all clean sexist tweets\n",
    "span_clean_train_sexist = [span_X_train_clean[i] for i in range(len(span_Y_train)) if span_Y_train[i] == 1]\n",
    "\n",
    "\n",
    "span_clean_train_upsample = span_X_train_clean + span_clean_train_sexist\n",
    "span_clean_Y_upsample = span_Y_train + [1 for i in range(len(span_clean_train_sexist))]\n",
    "span_clean_train_upsample_IDs = span_train_IDs + span_train_sexist_IDs\n",
    "\n",
    "# Check that it actually worked\n",
    "print('Sexist before upsampling:', span_Y_train.count(1))\n",
    "print('Not sexist before upsampling:', span_Y_train.count(0))\n",
    "print('Sexist after upsampling:', span_clean_Y_upsample.count(1))\n",
    "print('Not sexist after upsampling:', span_clean_Y_upsample.count(0))\n",
    "\n",
    "\n",
    "# Shuffle the Data\n",
    "span_clean_train_upsample, span_clean_Y_upsample, span_clean_train_upsample_IDs = shuffle(span_clean_train_upsample, span_clean_Y_upsample, span_clean_train_upsample_IDs, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the Spanish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [2 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 0 0]\n",
      " [3 2 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CLEAN\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "import numpy as np \n",
    "from scipy.sparse import hstack\n",
    "\n",
    "span_clean_tweets_train_upsampled = [data[0] for data in span_clean_train_upsample]\n",
    "span_clean_feats_train_upsampled = np.array([list(data[1:]) for data in span_clean_train_upsample])\n",
    "span_clean_tweets_test = [data[0] for data in span_X_test_clean]\n",
    "span_clean_feats_test = np.array([list(data[1:]) for data in span_X_test_clean])\n",
    "print(span_clean_feats_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training data shape: (5686, 10860)\n",
      "Clean testing data shape: (549, 10860)\n",
      "Train shape after feats added: (5686, 10864)\n",
      "Test shape after feats added: (549, 10864)\n"
     ]
    }
   ],
   "source": [
    "clean_Tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "\n",
    "# Fit the vectorizer on the clean training data\n",
    "clean_Tfidf.fit(span_clean_tweets_train_upsampled)\n",
    "\n",
    "# Transform the training and testing data using the vectorizer\n",
    "X_train_clean_tfidf = clean_Tfidf.transform(span_clean_tweets_train_upsampled)\n",
    "X_test_clean_tfidf = clean_Tfidf.transform(span_clean_tweets_test)\n",
    "\n",
    "\n",
    "print('Clean training data shape:', X_train_clean_tfidf.shape)\n",
    "print('Clean testing data shape:', X_test_clean_tfidf.shape)\n",
    "\n",
    "\n",
    "#Add the feats in\n",
    "X_train_clean_tfidf_feats = hstack([X_train_clean_tfidf, span_clean_feats_train_upsampled])\n",
    "X_test_clean_tfidf_feats = hstack([X_test_clean_tfidf, span_clean_feats_test])\n",
    "\n",
    "print('Train shape after feats added:', X_train_clean_tfidf_feats.shape)\n",
    "print('Test shape after feats added:', X_test_clean_tfidf_feats.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Spanish Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6886    0.6856    0.6871       229\n",
      "           1     0.7757    0.7781    0.7769       320\n",
      "\n",
      "    accuracy                         0.7395       549\n",
      "   macro avg     0.7321    0.7319    0.7320       549\n",
      "weighted avg     0.7394    0.7395    0.7394       549\n",
      "\n",
      "Accuracy: 0.7395264116575592\n"
     ]
    }
   ],
   "source": [
    "# CLEAN MODEL\n",
    "# Perform hyperparameter optimization and train the models\n",
    "\n",
    "clean_svm = SVC()\n",
    "\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "clean_svm_gs = GridSearchCV(clean_svm, params)\n",
    "clean_svm_gs.fit(X_train_clean_tfidf, span_clean_Y_upsample)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters:\", clean_svm_gs.best_params_)\n",
    "\n",
    "# Predict the labels of the test data using the trained classifier with the best hyperparameters\n",
    "clean_y_pred = clean_svm_gs.predict(X_test_clean_tfidf)\n",
    "\n",
    "# Print the classification report and accuracy score with the best hyperparameters\n",
    "print(classification_report(span_Y_test, clean_y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(span_Y_test, clean_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
